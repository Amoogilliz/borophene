import numpy as np
import torch
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import xgboost as xgb
import pennylane as qml
from torch import nn
from tqdm import tqdm
import gc
import webbrowser
from pathlib import Path
import time

np.random.seed(42)
torch.manual_seed(42)

print("Working directory:", os.getcwd())

n_samples = 5000
X = np.random.rand(n_samples, 3)
X[:, 0] *= 5.0
X[:, 1] = X[:, 1] * 8.0 + 2.0
X[:, 2] *= 0.5
y = -1.0 * (0.5 * X[:, 0]**2 - 0.3 * X[:, 1] + 2.0 * X[:, 2]**0.5) / 2.0
y += np.random.normal(0, 0.2, n_samples)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

n_qubits = 4
n_layers = 6
dev = qml.device("default.qubit", wires=n_qubits)

@qml.qnode(dev, interface="torch", diff_method="parameter-shift")
def quantum_circuit(inputs, weights):
    qml.AngleEmbedding(inputs, wires=range(n_qubits))
    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))
    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]

class HybridModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.q_params = nn.Parameter(0.01 * torch.randn(n_layers, n_qubits))
        self.classical_layer = nn.Linear(n_qubits, 1)
        
    def forward(self, x):
        batch_size = 16
        outputs = []
        for i in range(0, len(x), batch_size):
            batch = x[i:i+batch_size]
            q_out_batch = []
            for xi in batch:
                q_out_batch.append(torch.tensor(quantum_circuit(xi, self.q_params), dtype=torch.float32))
            q_out = torch.stack(q_out_batch)
            batch_output = self.classical_layer(q_out)
            outputs.append(batch_output)
            if i % 160 == 0 and i > 0:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        return torch.cat(outputs, dim=0)

def process_in_chunks(model, data, chunk_size=500):
    results = []
    for i in range(0, len(data), chunk_size):
        chunk = data[i:i+chunk_size]
        with torch.no_grad():
            chunk_result = model(chunk).detach().cpu().numpy()
        results.append(chunk_result)
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    return np.vstack(results)

model = HybridModel()
optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
loss_fn = nn.MSELoss()

X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_torch = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)
X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)

batch_size = 64
losses = []
epochs = 100
patience = 10
best_loss = float('inf')
patience_counter = 0

chunk_size = 1000
train_chunks = [(X_train_torch[i:i+chunk_size], y_train_torch[i:i+chunk_size]) 
                for i in range(0, len(X_train_torch), chunk_size)]

print("Training Quantum-Hybrid Model...")
for epoch in tqdm(range(epochs), desc="Epochs"):
    model.train()
    epoch_losses = []
    for chunk_idx, (X_chunk, y_chunk) in enumerate(train_chunks):
        chunk_losses = []
        for i in range(0, len(X_chunk), batch_size):
            X_batch = X_chunk[i:i+batch_size]
            y_batch = y_chunk[i:i+batch_size]
            optimizer.zero_grad()
            preds = model(X_batch)
            loss = loss_fn(preds, y_batch)
            loss.backward()
            optimizer.step()
            chunk_losses.append(loss.item())
        avg_chunk_loss = sum(chunk_losses) / len(chunk_losses)
        epoch_losses.append(avg_chunk_loss)
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    avg_loss = sum(epoch_losses) / len(epoch_losses)
    losses.append(avg_loss)
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}")
    if avg_loss < best_loss:
        best_loss = avg_loss
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

print("Training XGBoost Model...")
xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    tree_method='hist',
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

chunk_size = 2500
for i in range(0, len(X_train_scaled), chunk_size):
    end_idx = min(i + chunk_size, len(X_train_scaled))
    if i == 0:
        xgb_model.fit(X_train_scaled[i:end_idx], y_train[i:end_idx])
    else:
        xgb_model.fit(
            X_train_scaled[i:end_idx], 
            y_train[i:end_idx],
            xgb_model=xgb_model.get_booster(),
            eval_set=[(X_train_scaled[i:end_idx], y_train[i:end_idx])],
            verbose=False
        )
    gc.collect()

print("Running model inference...")
model.eval()
quantum_preds = process_in_chunks(model, X_test_torch).flatten()
xgb_preds = xgb_model.predict(X_test_scaled)

quantum_mae = mean_absolute_error(y_test, quantum_preds)
quantum_rmse = np.sqrt(mean_squared_error(y_test, quantum_preds))
quantum_r2 = r2_score(y_test, quantum_preds)

xgb_mae = mean_absolute_error(y_test, xgb_preds)
xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_preds))
xgb_r2 = r2_score(y_test, xgb_preds)

print("\nModel Performance Metrics:")
print(f"Quantum Model - MAE: {quantum_mae:.6f} eV/atom, RMSE: {quantum_rmse:.6f} eV/atom, R²: {quantum_r2:.4f}")
print(f"XGBoost Model - MAE: {xgb_mae:.6f} eV/atom, RMSE: {xgb_rmse:.6f} eV/atom, R²: {xgb_r2:.4f}")

print("Generating visualizations...")
quantum_residuals = quantum_preds - y_test
xgb_residuals = xgb_preds - y_test

sample_size = 500
sample_indices = np.random.choice(len(y_test), sample_size, replace=False)

del X_train_torch, y_train_torch
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

vis_dir = os.getcwd()
residual_path = os.path.join(vis_dir, 'residual_analysis.png')
training_path = os.path.join(vis_dir, 'training_and_predictions.png')
metrics_path = os.path.join(vis_dir, 'metrics_comparison.png')

try:
    plt.figure(figsize=(20, 16))
    plt.subplot(2, 2, 1)
    plt.hist(quantum_residuals[sample_indices], bins=20, alpha=0.7, label="Quantum Residuals")
    plt.hist(xgb_residuals[sample_indices], bins=20, alpha=0.7, label="XGBoost Residuals")
    plt.axvline(0, color='black', linestyle='--')
    plt.title("Residual Distribution (Sample)", fontsize=16)
    plt.xlabel("Prediction Error", fontsize=14)
    plt.ylabel("Frequency", fontsize=14)
    plt.legend(fontsize=12)
    plt.subplot(2, 2, 2)
    boxplot = plt.boxplot([quantum_residuals[sample_indices], xgb_residuals[sample_indices]], 
                labels=["Quantum", "XGBoost"])
    plt.title("Residual Spread by Model (Sample)", fontsize=16)
    plt.ylabel("Prediction Error", fontsize=14)
    plt.subplot(2, 2, 3)
    plt.plot(losses, linewidth=2)
    plt.title("Quantum Model Training Loss", fontsize=16)
    plt.xlabel("Epoch", fontsize=14)
    plt.ylabel("Loss", fontsize=14)
    plt.subplot(2, 2, 4)
    bar_width = 0.35
    index = np.arange(3)
    quantum_performance = [quantum_mae, quantum_rmse, quantum_r2]
    xgb_performance = [xgb_mae, xgb_rmse, xgb_r2]
    bars1 = plt.bar(index - bar_width / 2, quantum_performance, bar_width, 
                   label='Quantum', alpha=0.7, color='royalblue')
    bars2 = plt.bar(index + bar_width / 2, xgb_performance, bar_width, 
                   label='XGBoost', alpha=0.7, color='darkorange')
    plt.xlabel('Metric', fontsize=14)
    plt.ylabel('Value', fontsize=14)
    plt.title('Model Comparison: MAE, RMSE, R² Score', fontsize=16)
    plt.xticks(index, ['MAE', 'RMSE', 'R²'], fontsize=12)
    plt.legend(fontsize=12)
    def add_value_labels(bars):
        for bar in bars:
            height = bar.get_height()
            plt.annotate(f'{height:.4f}',
                         xy=(bar.get_x() + bar.get_width() / 2, height),
                         xytext=(0, 3),
                         textcoords="offset points",
                         ha='center', va='bottom',
                         fontsize=10)
    add_value_labels(bars1)
    add_value_labels(bars2)
    plt.suptitle("Hybrid Quantum-Classical ML for Predicting Borophene Formation Energy", fontsize=20)
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    combined_path = os.path.join(vis_dir, 'combined_results.png')
    plt.savefig(combined_path, dpi=300)
    print(f"Saved combined visualization to {combined_path}")
    try:
        webbrowser.open('file://' + os.path.abspath(combined_path))
        print(f"Opening combined visualization automatically...")
    except Exception as e:
        print(f"Could not open visualization automatically: {e}")
    plt.close()
except Exception as e:
    print(f"Error creating combined visualization: {e}")
